---
title: "Final DEA"
author: "Taylor Mack"
date: "8/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
The following code chunk will load all of the needed libraries used on the 
analysis

```{r}
library(plyr)
library(purrr)
library(dplyr)
library(tidyverse)
library(lubridate)
library(fixest)
```

## Data Sources

The following code chunk will create the various data sources for the analysis

```{r}
setwd('./Data/')
trend_data <- list.files(pattern = 'trends_up_to_', full.names = TRUE) %>%
           map_df(read_csv) %>%
           bind_rows()
scorecard_dict <- read_csv('./CollegeScorecardDataDictionary-09-08-2015.csv')
id_name_link <- read_csv('./id_name_link.csv')
most_recent_cohorts <- read_csv('./Most+Recent+Cohorts+(Scorecard+Elements).csv')
```
## Formatting the 'monthorweek' field

Parsing down the 'monthorweek' field from a system datetime to a yyyy-mm-dd format,
then rounding down every date to be the first of each month since the analysis is at the
yyyy-mm level. Also renaming 'monthorweek' to 'month' to be more indicative of what the data is

```{r}
trend_data$monthorweek <- str_sub(trend_data$monthorweek,1,10) %>%
                          ymd() %>%
                          floor_date(unit= 'month')
colnames(trend_data)[5] <- "month"
```

## Standardizing the index field

```{r}
trend_data <- trend_data %>% group_by(schname,keyword) %>%
              mutate(standard_index = (index - mean(index))/sd(index))
```

## Removing Colleges that have the same 'schname' value. 

```{r}
id_name_link <- group_by(id_name_link,schname) %>%
                mutate(n = n()) %>%
                filter(n == 1)
id_name_link <- id_name_link[,-4]
```

## Joining all of the datasets together

```{r}
trend_data <- inner_join(trend_data,id_name_link,by= "schname")
trend_data <- inner_join(trend_data,most_recent_cohorts,by= c("unitid"="UNITID","opeid"="OPEID"))
```

## Cleaning the datasource
  Activities
  a. Adding a column that is a count of all of the rows for each month
  b. Adding a column for the month number
  c. Formatting month to no longer include the date field
  d. Adding the "median_salary" field to be a mirror of the student's medium wage 10 years after        graduation
  e. Creating a binary field for when the scorecard went live, 0 being before 1 being after
  f. Filtered out non-numeric medium datapoints
  g. Creating a column to put incomes into "low", "medium" and "high" groupings
  h. filtered out any 'NA's that potentially were still in the dataset on the 'median_salary'
     field 
  
  Justification
  a. I wanted to have a column that was a count of all the activity for each school and keyword         that was aggregated for each month.
  b. This could be a useful to have month number seperated if anyone were to do a seasonality
     regression.
  c. Since I used the floor_date on month field it was misleading to have all of the data display       like it occurred on the first of the month. Formatting month to be "yyyy-mm" would eliminate
     that potential misunderstanding.
  d. I wanted to created a mirror field of the "md_earn_wne_p10-REPORTED-EARNINGS" field, because
     1) I didn't want to type out that long field more than once
     2) If I were to do any sort of transformation I didn't want to touch imported data
     I felt that renaming that field would cause more confusion and users wouldn't be able
     to rely on the 'scorecard_dict' for more info regarding the field.
  e. I wanted to have a binary that made it clear when the scorecard went live, with the ultimate
     goal of using that variable in my regression.
  f. I discovered that there were non-numerical values in the "median_salary" field, including 
     these text values ultimately skewed my regression model. I thought that the easiest way to 
     remove those values would be to try and convert the column into a numeric, so the lines that
     errored out were the text values. Since I was using the error out method I included a 
     'suppressWarnings()' function to not bombard users with error messages.
  g. I wanted to group the median salaries for each college into 3 ranges: low, medium, and high.
     The ranges were:
     Low : 0 - 30,000
     Medium : 30,001 - 74,999
     high : 75,000+
     I found that there some other fields that followed the above ranges. I also found that these       ranges were fairly common in the workplace. I also found that the middle class is considered
     to be 30,000 to around 75,000-90,000, but for my analysis I wanted to be slightly                  conservative, so I went with the lowest value on the top end of the middle class. 
  h. I threw on another na filter just to catch anything that wasn't caught from the filter on 
     step f.

```{r}
trend_data <- trend_data %>% group_by(schname,keyword,month) %>%
               mutate(n = n()) %>%
               mutate(year = year(month)) %>%
               mutate(month_number = month(month)) %>%
               mutate(month = format(month,"%Y-%m")) %>%
               mutate(median_salary = `md_earn_wne_p10-REPORTED-EARNINGS`) %>%
               mutate(scorecard_live = case_when
               (month < '2015-09'~ 0,
                month >= '2015-09' ~ 1)) %>%
               filter(!is.na(suppressWarnings(as.numeric(median_salary)))) %>%
               mutate(income_category = case_when
               (median_salary <= 30000 ~ "low",
                median_salary > 30000 &
                median_salary < 75000 ~ "medium",
                median_salary >= 75000 ~ "high")) %>%
               filter(is.na(median_salary) == FALSE)
```

## Plotting the summarized 'n' variable by month

I wanted to take a look at the total population for search activity, field 'n', to see if I could
spot any oddities with the data to help me identify if I needed to add any constraints to the 
regression. The 'n' values seemed sporadic and violent movement, I felt that I would need to throw a log on the n field to help smooth out the regression. Also the ggplot resembled a log graph, with a lot of noise, because of the intense upswing at the end of the data plots.

```{r}
trend_data %>% group_by(month,schname,keyword,income_category) %>%
  summarize(total_search_activity = sum(n)) %>%
  ggplot(mapping = aes(x = month,y= total_search_activity, group= income_category)) +
  geom_line(aes(color=income_category)) + geom_point() 
```

## Plotting the average standard index for each school and keyword

I ultimately planned on using standard index in my regression, and I wanted to see what the treatment could be. Since there are negative values I threw out logs as an option of handling 
standard_index. It looked like a linear relationship would be the best fit, but I wanted to try 
a few different methods to see what would be the most effective. All of the data points are hovering(+/-) an average, and would lead me to believe that I am not going to have add additional functions for the treatment of standard_index.

```{r}
trend_data %>% group_by(month,schname,keyword,income_category) %>%
  summarize(avg_std_index = mean(standard_index)) %>%  
  ggplot(mapping = aes(x = month,y= avg_std_index, group= income_category)) +
  geom_line(aes(color=income_category)) + geom_point() 
```

## The Regressions

Below are the regressions that I ran, and I tried a few different methods on how to treat standard_index, but will ultimately go with 'reg' as my primary regression. 'reg2' seemed to have a mildly worst fit that 'reg', and 'reg3' should be avoid since it resulted in collinearity on the 'income_categorymedium' variable.

Our question was if the release of the scorecard shifted interest to high-earning schools compared to low-earning schools. To answer this question our dependent variable will be 'n', but based on what I indicated above we would need our dependent variable to actually be log(n). Since the key relationship we want to see is if the scorecard impact 'income_category', thus I was interested in the interaction between the two variables, which is why on my regression I used 'income_category*scorecard_live' which takes the low, medium, and high variables and has them interact with the binary variable indicating whether or not the scorecard was live yet. Lastly, I was interested in the interaction of the standard_index on the model. 

Results:
What I found is that after the scorecard was live there was not a shift in interest for high median income schools, but instead saw low/medium income schools gaining a larger percent increase in interest. After the scorecard was live low income schools saw an increase in attention by .29, while medium income schools only saw a .23 increase. 



```{r}
reg <- feols(data= trend_data,log(n)~income_category*scorecard_live + standard_index,vcov = 'hetero')
reg2  <- feols(data= trend_data,log(n)~income_category*scorecard_live + I(standard_index^2),vcov = 'hetero')
reg3 <- feols(data= trend_data,log(n)~income_category*scorecard_live | standard_index,vcov = 'hetero')
etable(reg,reg2,reg3)
wald(reg)
wald(reg2)
wald(reg3)
```

